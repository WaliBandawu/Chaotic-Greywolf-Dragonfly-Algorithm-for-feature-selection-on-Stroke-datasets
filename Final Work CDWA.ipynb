{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a86700-f34d-4dbc-b47c-51f7b1fab8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from CGWODA import HybridOptimizationAlgorithm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from zoofs import DragonFlyOptimization\n",
    "\n",
    "from zoofs import GreyWolfOptimization\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from zoofs.baseoptimizationalgorithm import BaseOptimizationAlgorithm\n",
    "from zoofs import DragonFlyOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20293dc-923e-426f-aab8-9fc403dcfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "def objective_function_topass( model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    model.fit(X_train, y_train) \n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Calculate the classification error rate\n",
    "    error_rate = 1 - accuracy\n",
    "    cardinality_R = X_train.shape[1]\n",
    "    # Calculate the total number of features in C\n",
    "    total_features_C = 10\n",
    "    alpha = 0.99\n",
    "    # Calculate beta\n",
    "    beta = 1 - alpha\n",
    "    # Calculate the fitness value\n",
    "    fitness = error_rate * alpha + beta * (cardinality_R / total_features_C)\n",
    "    return fitness\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9c579-3c44-4d38-a869-a6492b5c7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cgwoda_optimization(num_iterations, X_train, y_train, X_test, y_test):\n",
    "    results_per_iteration = []\n",
    "    best_feature_lists = []\n",
    "    best_dims = []\n",
    "    best_scores = []\n",
    "\n",
    "    for _ in tqdm(range(num_iterations), desc=\"Iterations\", unit=\"iteration\"):\n",
    "        hybrid_object = HybridOptimizationAlgorithm(\n",
    "            objective_function=objective_function_topass,\n",
    "            n_iteration=100,\n",
    "            population_size=11,\n",
    "            logger=None,\n",
    "        )\n",
    "        \n",
    "        knn_classifier = KNeighborsClassifier()\n",
    "        \n",
    "        hybrid_object.fit(knn_classifier, X_train, y_train, X_test, y_test, verbose=False)\n",
    "        \n",
    "        \n",
    "        iteration_results = {\n",
    "            \"results_per_iteration\": hybrid_object.best_results_per_iteration,\n",
    "            \"best_dim_list\": hybrid_object.best_dim.tolist(),\n",
    "            \"best_feature_list\": hybrid_object.best_feature_list,\n",
    "            \"best_score\": hybrid_object.best_score\n",
    "        }\n",
    "        \n",
    "        best_feature_list = hybrid_object.best_feature_list\n",
    "        best_dim = hybrid_object.best_dim\n",
    "        best_score = hybrid_object.best_score\n",
    "        \n",
    "        results_per_iteration.append(iteration_results)\n",
    "        best_feature_lists.append(best_feature_list)\n",
    "        best_dims.append(best_dim.tolist())\n",
    "        best_scores.append(best_score)\n",
    "    \n",
    "    return results_per_iteration, best_feature_lists, best_dims, best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25405d6-3898-4e40-b77b-4c122bf2ef4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf82c27c-ae3b-466a-a0b5-72b38d91f8e3",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9998b6a-f297-46b6-9115-9dee13bedafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "        # Separate the features and target variable\n",
    "        if 'id' in dataset.columns:\n",
    "            dataset = dataset.drop('id', axis=1)\n",
    "\n",
    "        # Perform label encoding on categorical features\n",
    "        categorical_features = dataset.select_dtypes(include=['object']).columns\n",
    "        label_encoder = LabelEncoder()\n",
    "        for feature in categorical_features:\n",
    "            dataset[feature] = label_encoder.fit_transform(dataset[feature])\n",
    "\n",
    "        dataset['bmi'].fillna(dataset['bmi'].mean(), inplace=True)\n",
    "        \n",
    "        # Normalize the data\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized = scaler.fit_transform(dataset)\n",
    "\n",
    "        # Return the preprocessed dataset\n",
    "        preprocessed_dataset = pd.DataFrame(dataset, columns=dataset.columns)\n",
    "        return preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deea3586-d062-4290-ba21-ad6588c78f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke=pd.read_csv('stroke4.csv')\n",
    "stroke=preprocess_dataset(stroke)\n",
    "X = stroke.drop(['stroke'],axis=1)\n",
    "y = stroke['stroke']\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Assuming X and y are your feature matrix and target variable, respectively\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eda7b3f-d4f7-4f90-bc79-8bc9b1e541b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   7%|▋         | 2/30 [01:40<23:26, 50.24s/iteration]WARNING:root:1 individuals went zero\n",
      "Iterations:  13%|█▎        | 4/30 [03:19<21:21, 49.29s/iteration]WARNING:root:1 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "Iterations:  30%|███       | 9/30 [07:51<18:19, 52.35s/iteration]WARNING:root:1 individuals went zero\n",
      "Iterations:  33%|███▎      | 10/30 [08:43<17:22, 52.12s/iteration]WARNING:root:1 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "Iterations:  40%|████      | 12/30 [10:31<15:52, 52.89s/iteration]WARNING:root:2 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "Iterations:  53%|█████▎    | 16/30 [14:15<12:53, 55.24s/iteration]WARNING:root:1 individuals went zero\n",
      "Iterations:  73%|███████▎  | 22/30 [19:36<07:09, 53.74s/iteration]WARNING:root:1 individuals went zero\n",
      "Iterations:  83%|████████▎ | 25/30 [22:15<04:26, 53.37s/iteration]WARNING:root:1 individuals went zero\n",
      "Iterations:  87%|████████▋ | 26/30 [23:02<03:25, 51.42s/iteration]WARNING:root:1 individuals went zero\n",
      "WARNING:root:1 individuals went zero\n",
      "WARNING:root:2 individuals went zero\n",
      "Iterations: 100%|██████████| 30/30 [26:20<00:00, 52.68s/iteration]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 30 \n",
    "results, features, dims, scores = run_cgwoda_optimization(num_iterations, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5a625-eede-4164-b197-51a6b68ac034",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba12bae-ac9f-46e5-9831-cf29791de317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gender', 'heart_disease', 'ever_married'],\n",
       " ['age', 'hypertension', 'ever_married'],\n",
       " ['age', 'heart_disease'],\n",
       " ['gender', 'hypertension', 'heart_disease'],\n",
       " ['gender', 'hypertension', 'heart_disease'],\n",
       " ['gender', 'ever_married'],\n",
       " ['gender', 'age', 'ever_married', 'avg_glucose_level', 'bmi'],\n",
       " ['heart_disease', 'ever_married'],\n",
       " ['gender', 'age', 'ever_married', 'avg_glucose_level', 'bmi'],\n",
       " ['gender', 'ever_married', 'work_type'],\n",
       " ['gender', 'hypertension', 'heart_disease', 'ever_married'],\n",
       " ['gender', 'hypertension', 'ever_married'],\n",
       " ['heart_disease', 'work_type', 'Residence_type'],\n",
       " ['gender', 'heart_disease'],\n",
       " ['age', 'ever_married'],\n",
       " ['gender', 'heart_disease', 'ever_married'],\n",
       " ['gender'],\n",
       " ['gender', 'heart_disease'],\n",
       " ['hypertension', 'heart_disease'],\n",
       " ['hypertension', 'heart_disease', 'Residence_type'],\n",
       " ['gender', 'heart_disease', 'ever_married'],\n",
       " ['gender', 'heart_disease'],\n",
       " ['gender', 'hypertension', 'ever_married'],\n",
       " ['gender', 'hypertension'],\n",
       " ['age', 'hypertension', 'ever_married'],\n",
       " ['hypertension'],\n",
       " ['gender', 'ever_married', 'Residence_type'],\n",
       " ['gender', 'ever_married'],\n",
       " ['age', 'hypertension'],\n",
       " ['age', 'heart_disease', 'ever_married', 'avg_glucose_level', 'bmi']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7858b5ba-ece3-4741-8cb7-c73a1f85285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gender', 'heart_disease', 'ever_married']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Convert sublists to tuples for counting\n",
    "tuple_list = [tuple(sublist) for sublist in features]\n",
    "counter = Counter(tuple_list)\n",
    "most_common =list( counter.most_common(1)[0][0])\n",
    "print(most_common)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4bc20-1ea8-4e1d-ad97-e0c3daa82e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6541c65-6b8b-4f33-81c3-dbcce8b78b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stroke_data =pd.DataFrame(X[['age', 'heart_disease', 'ever_married', 'avg_glucose_level', 'bmi']])\n",
    "Stroke_data['stroke']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d8210ec-a8e1-4a8e-9ba0-3e31ff0527d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20230821_110907\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230821_110907\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   33.12 GB / 255.46 GB (13.0%)\n",
      "Train Data Rows:    9722\n",
      "Train Data Columns: 5\n",
      "Label Column: stroke\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1313.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.35 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['age', 'avg_glucose_level', 'bmi']\n",
      "\t\t('int', [])   : 2 | ['heart_disease', 'ever_married']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 3 | ['age', 'avg_glucose_level', 'bmi']\n",
      "\t\t('int', ['bool']) : 2 | ['heart_disease', 'ever_married']\n",
      "\t0.1s = Fit runtime\n",
      "\t5 features in original data used to generate 5 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8749, Val Rows: 973\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.8993\t = Validation score   (accuracy)\n",
      "\t3.49s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9075\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.101747\n",
      "[2000]\tvalid_set's binary_error: 0.0883864\n",
      "[3000]\tvalid_set's binary_error: 0.0801644\n",
      "[4000]\tvalid_set's binary_error: 0.0719424\n",
      "[5000]\tvalid_set's binary_error: 0.0688592\n",
      "[6000]\tvalid_set's binary_error: 0.0678314\n",
      "[7000]\tvalid_set's binary_error: 0.0668037\n",
      "[8000]\tvalid_set's binary_error: 0.0647482\n",
      "[9000]\tvalid_set's binary_error: 0.0626927\n",
      "[10000]\tvalid_set's binary_error: 0.0626927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9394\t = Validation score   (accuracy)\n",
      "\t12.99s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.045221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9609\t = Validation score   (accuracy)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9301\t = Validation score   (accuracy)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9311\t = Validation score   (accuracy)\n",
      "\t2.15s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9661\t = Validation score   (accuracy)\n",
      "\t7.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9342\t = Validation score   (accuracy)\n",
      "\t1.08s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9301\t = Validation score   (accuracy)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.8571\t = Validation score   (accuracy)\n",
      "\t20.2s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9363\t = Validation score   (accuracy)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.8952\t = Validation score   (accuracy)\n",
      "\t76.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9568\t = Validation score   (accuracy)\n",
      "\t1.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9671\t = Validation score   (accuracy)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 136.28s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230821_110907\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor_combined = TabularPredictor(\n",
    "    label=\"stroke\", \n",
    "    \n",
    ").fit(\n",
    "    train_data=Stroke_data, \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842af3b-89e6-427f-941d-5dbd644277e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feadd4b-522e-4dd9-a0ba-7a1b5761ffdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d37f12b1-8aab-4ace-ab4e-b599bdb28aae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 0/30 [00:00<?, ?iteration/s]\u001b[32m [ 2023-08-21 11:26:02,849 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:   3%|▎         | 1/30 [01:12<34:52, 72.14s/iteration]\u001b[32m [ 2023-08-21 11:27:32,841 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:27:41,057 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:27:41,281 ] \u001b[0m2 individuals went zero\u001b[0m\n",
      "Iterations:   7%|▋         | 2/30 [02:22<33:04, 70.87s/iteration]\u001b[32m [ 2023-08-21 11:28:26,805 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:28:52,640 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  10%|█         | 3/30 [03:31<31:35, 70.19s/iteration]\u001b[32m [ 2023-08-21 11:29:17,312 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:29:22,635 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:29:26,358 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:29:52,146 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:29:55,970 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:29:57,145 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:29:59,969 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:30:00,697 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  13%|█▎        | 4/30 [04:40<30:11, 69.66s/iteration]\u001b[32m [ 2023-08-21 11:30:48,193 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:30:53,520 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:30:55,831 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:31:03,627 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:31:08,640 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  17%|█▋        | 5/30 [05:50<29:07, 69.88s/iteration]\u001b[32m [ 2023-08-21 11:31:22,388 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:31:44,561 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:32:04,248 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:32:20,702 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  20%|██        | 6/30 [07:02<28:11, 70.49s/iteration]\u001b[32m [ 2023-08-21 11:33:08,078 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  23%|██▎       | 7/30 [08:11<26:50, 70.02s/iteration]\u001b[32m [ 2023-08-21 11:34:27,913 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:34:39,200 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:34:43,643 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  27%|██▋       | 8/30 [09:24<26:03, 71.05s/iteration]\u001b[32m [ 2023-08-21 11:35:20,690 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:35:28,674 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:35:51,180 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:35:52,789 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:35:55,331 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:35:57,588 ] \u001b[0m2 individuals went zero\u001b[0m\n",
      "Iterations:  30%|███       | 9/30 [10:37<25:00, 71.47s/iteration]\u001b[32m [ 2023-08-21 11:36:19,113 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:36:43,573 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:36:45,039 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:37:03,931 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  33%|███▎      | 10/30 [11:43<23:15, 69.80s/iteration]\u001b[32m [ 2023-08-21 11:37:49,691 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:38:11,460 ] \u001b[0m2 individuals went zero\u001b[0m\n",
      "Iterations:  37%|███▋      | 11/30 [12:53<22:07, 69.87s/iteration]\u001b[32m [ 2023-08-21 11:38:36,594 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:38:42,276 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:38:50,828 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:38:58,353 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:39:07,620 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:39:14,518 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:39:19,311 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:39:22,123 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:39:22,733 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  40%|████      | 12/30 [14:02<20:56, 69.81s/iteration]\u001b[32m [ 2023-08-21 11:39:56,531 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:40:14,986 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:40:30,363 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:40:35,688 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:40:37,600 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  43%|████▎     | 13/30 [15:19<20:24, 72.06s/iteration]\u001b[32m [ 2023-08-21 11:41:07,503 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  47%|████▋     | 14/30 [16:36<19:36, 73.55s/iteration]\u001b[32m [ 2023-08-21 11:42:14,857 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  50%|█████     | 15/30 [17:45<17:59, 71.96s/iteration]\u001b[32m [ 2023-08-21 11:44:07,481 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  53%|█████▎    | 16/30 [18:53<16:33, 70.95s/iteration]\u001b[32m [ 2023-08-21 11:45:24,807 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  57%|█████▋    | 17/30 [20:12<15:54, 73.39s/iteration]\u001b[32m [ 2023-08-21 11:46:46,360 ] \u001b[0m2 individuals went zero\u001b[0m\n",
      "Iterations:  60%|██████    | 18/30 [21:25<14:39, 73.25s/iteration]\u001b[32m [ 2023-08-21 11:46:47,748 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:47:47,376 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  63%|██████▎   | 19/30 [22:35<13:15, 72.30s/iteration]\u001b[32m [ 2023-08-21 11:48:41,278 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:48:43,270 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:48:54,538 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  67%|██████▋   | 20/30 [23:46<11:58, 71.83s/iteration]\u001b[32m [ 2023-08-21 11:49:50,847 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  70%|███████   | 21/30 [24:57<10:44, 71.66s/iteration]\u001b[32m [ 2023-08-21 11:51:28,696 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:51:29,831 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:51:31,299 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  73%|███████▎  | 22/30 [26:10<09:34, 71.86s/iteration]\u001b[32m [ 2023-08-21 11:52:35,457 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:52:36,557 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  77%|███████▋  | 23/30 [27:18<08:14, 70.69s/iteration]\u001b[32m [ 2023-08-21 11:53:00,578 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:53:02,387 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:53:41,154 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:53:45,617 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:53:46,698 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  80%|████████  | 24/30 [28:25<06:57, 69.58s/iteration]\u001b[32m [ 2023-08-21 11:54:46,958 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:54:50,606 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  83%|████████▎ | 25/30 [29:33<05:46, 69.31s/iteration]\u001b[32m [ 2023-08-21 11:55:03,436 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:55:47,573 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:56:01,734 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:56:03,595 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:56:05,704 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  87%|████████▋ | 26/30 [30:47<04:42, 70.52s/iteration]\u001b[32m [ 2023-08-21 11:56:09,116 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:56:53,200 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:57:07,966 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:57:14,140 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  90%|█████████ | 27/30 [31:56<03:30, 70.07s/iteration]\u001b[32m [ 2023-08-21 11:57:31,820 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:58:06,546 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 11:58:10,006 ] \u001b[0m2 individuals went zero\u001b[0m\n",
      "Iterations:  93%|█████████▎| 28/30 [33:09<02:21, 70.93s/iteration]\u001b[32m [ 2023-08-21 11:59:12,605 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  97%|█████████▋| 29/30 [34:23<01:11, 71.99s/iteration]\u001b[32m [ 2023-08-21 12:00:28,166 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:00:40,994 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:00:50,137 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations: 100%|██████████| 30/30 [35:31<00:00, 71.06s/iteration]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 30\n",
    "da_results, da_features, da_dims, da_scores = run_da_optimization(num_iterations, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5703561e-eb63-4b5f-9a89-c716f8b99987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age', 'hypertension', 'ever_married'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age'],\n",
       " ['age']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "345bc117-767d-44d1-b112-4510ad2d98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stroke_data =pd.DataFrame(X['age'])\n",
    "Stroke_data['stroke']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac322a66-f07a-400c-94e5-c0cca22570f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20230821_120536\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230821_120536\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   32.89 GB / 255.46 GB (12.9%)\n",
      "Train Data Rows:    9722\n",
      "Train Data Columns: 1\n",
      "Label Column: stroke\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1791.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.08 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1 | ['age']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1 | ['age']\n",
      "\t0.1s = Fit runtime\n",
      "\t1 features in original data used to generate 1 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8749, Val Rows: 973\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9291\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9322\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.7873\t = Validation score   (accuracy)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9147\t = Validation score   (accuracy)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9332\t = Validation score   (accuracy)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9332\t = Validation score   (accuracy)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9168\t = Validation score   (accuracy)\n",
      "\t3.89s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9332\t = Validation score   (accuracy)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9332\t = Validation score   (accuracy)\n",
      "\t0.97s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.7852\t = Validation score   (accuracy)\n",
      "\t10.35s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.8643\t = Validation score   (accuracy)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7842\t = Validation score   (accuracy)\n",
      "\t16.35s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9147\t = Validation score   (accuracy)\n",
      "\t1.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9342\t = Validation score   (accuracy)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 40.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230821_120536\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor_combined = TabularPredictor(\n",
    "    label=\"stroke\", \n",
    "    \n",
    ").fit(\n",
    "    train_data=Stroke_data, \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "875c2233-421c-4bff-b0eb-6d202828c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stroke_data =pd.DataFrame(X[['age', 'hypertension', 'ever_married']])\n",
    "Stroke_data['stroke']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60612437-9588-42e6-be1f-56238558960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20230821_120837\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230821_120837\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   32.84 GB / 255.46 GB (12.9%)\n",
      "Train Data Rows:    9722\n",
      "Train Data Columns: 3\n",
      "Label Column: stroke\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1551.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.19 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1 | ['age']\n",
      "\t\t('int', [])   : 2 | ['hypertension', 'ever_married']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 1 | ['age']\n",
      "\t\t('int', ['bool']) : 2 | ['hypertension', 'ever_married']\n",
      "\t0.1s = Fit runtime\n",
      "\t3 features in original data used to generate 3 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.1 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8749, Val Rows: 973\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9291\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9322\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.7903\t = Validation score   (accuracy)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9301\t = Validation score   (accuracy)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9373\t = Validation score   (accuracy)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9373\t = Validation score   (accuracy)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9301\t = Validation score   (accuracy)\n",
      "\t4.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9373\t = Validation score   (accuracy)\n",
      "\t0.82s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9373\t = Validation score   (accuracy)\n",
      "\t0.84s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 8: early stopping\n",
      "\t0.7934\t = Validation score   (accuracy)\n",
      "\t12.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.8767\t = Validation score   (accuracy)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7914\t = Validation score   (accuracy)\n",
      "\t17.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.927\t = Validation score   (accuracy)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9435\t = Validation score   (accuracy)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.12s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230821_120837\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor_combined = TabularPredictor(\n",
    "    label=\"stroke\", \n",
    "    \n",
    ").fit(\n",
    "    train_data=Stroke_data, \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be9211-cc88-466c-8de2-907a5bdff948",
   "metadata": {},
   "source": [
    "## DataSet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4f5d20d-ff6a-4fcb-b5f2-09c5e6a7d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke2 = pd.read_csv('stroke/stroke2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5006ca31-06e9-46fd-b0f6-97c47f272e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke2=preprocess_dataset(stroke2)\n",
    "X = stroke2.drop(['stroke'],axis=1)\n",
    "y = stroke2['stroke']\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Assuming X and y are your feature matrix and target variable, respectively\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6e3cd6-7ae6-4d8c-9adf-895ebe5ec714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b754ed5d-cf62-4076-abdc-a06238cfa817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   3%|▎         | 1/30 [00:43<20:55, 43.29s/iteration]\u001b[32m [ 2023-08-21 12:16:02,505 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:   7%|▋         | 2/30 [01:28<20:42, 44.39s/iteration]\u001b[32m [ 2023-08-21 12:16:47,227 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:16:51,298 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  13%|█▎        | 4/30 [03:01<19:40, 45.40s/iteration]\u001b[32m [ 2023-08-21 12:18:41,181 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  23%|██▎       | 7/30 [05:25<18:03, 47.12s/iteration]\u001b[32m [ 2023-08-21 12:21:07,383 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:21:16,765 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  30%|███       | 9/30 [07:06<16:56, 48.39s/iteration]\u001b[32m [ 2023-08-21 12:22:22,053 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  37%|███▋      | 11/30 [08:39<15:06, 47.73s/iteration]\u001b[32m [ 2023-08-21 12:24:19,125 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  43%|████▎     | 13/30 [10:13<13:22, 47.22s/iteration]\u001b[32m [ 2023-08-21 12:25:24,283 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  53%|█████▎    | 16/30 [12:38<11:11, 47.96s/iteration]\u001b[32m [ 2023-08-21 12:27:45,273 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  67%|██████▋   | 20/30 [15:40<07:39, 45.91s/iteration]\u001b[32m [ 2023-08-21 12:31:01,161 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  70%|███████   | 21/30 [16:27<06:56, 46.24s/iteration]\u001b[32m [ 2023-08-21 12:31:56,806 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  77%|███████▋  | 23/30 [17:58<05:20, 45.83s/iteration]\u001b[32m [ 2023-08-21 12:33:08,295 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  80%|████████  | 24/30 [18:47<04:41, 46.84s/iteration]\u001b[32m [ 2023-08-21 12:34:14,295 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  90%|█████████ | 27/30 [21:04<02:18, 46.25s/iteration]\u001b[32m [ 2023-08-21 12:36:32,818 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:36:36,832 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:36:47,356 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:36:49,830 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations: 100%|██████████| 30/30 [23:26<00:00, 46.90s/iteration]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 30 \n",
    "results, features, dims, scores = run_cgwoda_optimization(num_iterations, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc9e6f8b-42fd-4be7-ad58-d5fcb46d1c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gender', 'Residence_type'],\n",
       " ['age', 'heart_disease', 'ever_married'],\n",
       " ['age', 'ever_married', 'work_type'],\n",
       " ['gender', 'ever_married', 'work_type', 'Residence_type'],\n",
       " ['age', 'work_type'],\n",
       " ['gender', 'heart_disease'],\n",
       " ['gender'],\n",
       " ['hypertension', 'heart_disease', 'ever_married'],\n",
       " ['heart_disease', 'avg_glucose_level'],\n",
       " ['gender', 'hypertension', 'ever_married'],\n",
       " ['gender', 'ever_married'],\n",
       " ['age', 'avg_glucose_level', 'bmi'],\n",
       " ['gender', 'ever_married'],\n",
       " ['gender', 'heart_disease', 'Residence_type'],\n",
       " ['gender', 'age', 'ever_married', 'avg_glucose_level', 'bmi'],\n",
       " ['gender', 'heart_disease', 'Residence_type'],\n",
       " ['age', 'work_type'],\n",
       " ['gender', 'ever_married', 'work_type', 'Residence_type'],\n",
       " ['age', 'avg_glucose_level', 'bmi'],\n",
       " ['gender', 'ever_married', 'work_type', 'Residence_type'],\n",
       " ['age', 'heart_disease', 'Residence_type'],\n",
       " ['gender', 'heart_disease', 'Residence_type'],\n",
       " ['gender', 'heart_disease', 'Residence_type'],\n",
       " ['gender', 'heart_disease', 'Residence_type'],\n",
       " ['hypertension', 'heart_disease', 'ever_married', 'work_type'],\n",
       " ['ever_married'],\n",
       " ['age', 'ever_married', 'work_type'],\n",
       " ['hypertension', 'ever_married', 'smoking_status'],\n",
       " ['gender', 'heart_disease', 'Residence_type'],\n",
       " ['age', 'avg_glucose_level', 'bmi']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11a7e5-2f22-4fc3-810c-c1a23ca0efd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e8730e9-ebb4-4660-9633-54ffa1064c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stroke_data =pd.DataFrame(X[['gender', 'age', 'ever_married', 'avg_glucose_level', 'bmi']])\n",
    "Stroke_data['stroke']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a1b396d-7091-480c-b743-7723c0a848f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20230821_124028\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230821_124028\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   32.87 GB / 255.46 GB (12.9%)\n",
      "Train Data Rows:    9466\n",
      "Train Data Columns: 5\n",
      "Label Column: stroke\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1650.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.3 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['age', 'avg_glucose_level', 'bmi']\n",
      "\t\t('int', [])   : 2 | ['gender', 'ever_married']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 3 | ['age', 'avg_glucose_level', 'bmi']\n",
      "\t\t('int', ['bool']) : 2 | ['gender', 'ever_married']\n",
      "\t0.1s = Fit runtime\n",
      "\t5 features in original data used to generate 5 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.25 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8519, Val Rows: 947\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.8807\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.8902\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.123548\n",
      "[2000]\tvalid_set's binary_error: 0.101373\n",
      "[3000]\tvalid_set's binary_error: 0.0918691\n",
      "[4000]\tvalid_set's binary_error: 0.0887012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9124\t = Validation score   (accuracy)\n",
      "\t5.37s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9493\t = Validation score   (accuracy)\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9219\t = Validation score   (accuracy)\n",
      "\t1.36s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9145\t = Validation score   (accuracy)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9398\t = Validation score   (accuracy)\n",
      "\t6.89s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9134\t = Validation score   (accuracy)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9092\t = Validation score   (accuracy)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.8332\t = Validation score   (accuracy)\n",
      "\t11.88s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9271\t = Validation score   (accuracy)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.8786\t = Validation score   (accuracy)\n",
      "\t68.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9514\t = Validation score   (accuracy)\n",
      "\t2.18s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9514\t = Validation score   (accuracy)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 106.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230821_124028\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor_combined = TabularPredictor(\n",
    "    label=\"stroke\", \n",
    "    \n",
    ").fit(\n",
    "    train_data=Stroke_data, \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac28038d-b513-4397-9b33-9ef80979ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def run_da_optimization(num_iterations, X_train, y_train, X_test, y_test):\n",
    "    da_results_per_iteration = []\n",
    "    da_best_feature_lists = []\n",
    "    da_best_dims = []\n",
    "    da_best_scores = []\n",
    "    \n",
    "    for _ in tqdm(range(num_iterations), desc=\"Iterations\", unit=\"iteration\"):\n",
    "        # Replace with your actual algorithm import and instantiation\n",
    "        algo_object = DragonFlyOptimization(objective_function_topass,\n",
    "                                            n_iteration=100,\n",
    "                                            population_size=11,\n",
    "                                            minimize=True,\n",
    "                                            logger=None)\n",
    "        \n",
    "        model = KNeighborsClassifier()\n",
    "        \n",
    "        algo_object.fit(model, X_train, y_train, X_test, y_test, verbose=False)\n",
    "    \n",
    "        da_iteration_results = {\n",
    "            \"results_per_iteration\": algo_object.best_results_per_iteration,\n",
    "            \"best_dim_list\": algo_object.best_dim.tolist(),\n",
    "            \"best_feature_list\": algo_object.best_feature_list,\n",
    "            \"best_score\": algo_object.best_score\n",
    "        }\n",
    "        da_best_feature_list = algo_object.best_feature_list\n",
    "        da_best_dim = algo_object.best_dim\n",
    "        da_best_score = algo_object.best_score\n",
    "\n",
    "        da_results_per_iteration.append(da_iteration_results)\n",
    "        da_best_feature_lists.append(da_best_feature_list)\n",
    "        da_best_dims.append(da_best_dim.tolist())\n",
    "        da_best_scores.append(da_best_score)\n",
    "    \n",
    "    return da_results_per_iteration, da_best_feature_lists, da_best_dims, da_best_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4dc7517-c14a-4603-9d61-12b0176372f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|          | 0/30 [00:00<?, ?iteration/s]\u001b[32m [ 2023-08-21 12:44:02,413 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:44:10,368 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:44:22,739 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:   3%|▎         | 1/30 [01:06<31:55, 66.06s/iteration]\u001b[32m [ 2023-08-21 12:45:54,641 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:   7%|▋         | 2/30 [02:14<31:31, 67.57s/iteration]\u001b[32m [ 2023-08-21 12:46:28,113 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:46:59,709 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:47:00,210 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:47:05,249 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  10%|█         | 3/30 [03:25<31:02, 68.99s/iteration]\u001b[32m [ 2023-08-21 12:47:10,926 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:47:14,544 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  13%|█▎        | 4/30 [04:33<29:46, 68.69s/iteration]\u001b[32m [ 2023-08-21 12:48:15,510 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:49:13,047 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:49:16,502 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:49:17,275 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  17%|█▋        | 5/30 [05:39<28:12, 67.70s/iteration]\u001b[32m [ 2023-08-21 12:50:06,996 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:50:29,154 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  20%|██        | 6/30 [06:48<27:17, 68.23s/iteration]\u001b[32m [ 2023-08-21 12:50:40,104 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:50:52,745 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:51:15,560 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:51:42,207 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  23%|██▎       | 7/30 [08:01<26:43, 69.71s/iteration]\u001b[32m [ 2023-08-21 12:52:38,409 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  27%|██▋       | 8/30 [09:12<25:40, 70.02s/iteration]\u001b[32m [ 2023-08-21 12:53:17,487 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:53:30,632 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:53:54,575 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  30%|███       | 9/30 [10:21<24:26, 69.82s/iteration]\u001b[32m [ 2023-08-21 12:54:43,714 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:55:07,670 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  33%|███▎      | 10/30 [11:29<23:05, 69.26s/iteration]\u001b[32m [ 2023-08-21 12:55:13,819 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:55:55,153 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:56:15,126 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:56:17,360 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  37%|███▋      | 11/30 [12:38<21:52, 69.07s/iteration]\u001b[32m [ 2023-08-21 12:56:42,466 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  40%|████      | 12/30 [13:49<20:54, 69.67s/iteration]\u001b[32m [ 2023-08-21 12:57:57,725 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:58:30,901 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  43%|████▎     | 13/30 [14:57<19:34, 69.11s/iteration]\u001b[32m [ 2023-08-21 12:59:05,681 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:59:26,054 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:59:30,340 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:59:33,020 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:59:45,042 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 12:59:46,496 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  47%|████▋     | 14/30 [16:07<18:32, 69.54s/iteration]\u001b[32m [ 2023-08-21 13:00:27,438 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  50%|█████     | 15/30 [17:20<17:36, 70.43s/iteration]\u001b[32m [ 2023-08-21 13:02:05,790 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  53%|█████▎    | 16/30 [18:29<16:20, 70.07s/iteration]\u001b[32m [ 2023-08-21 13:02:43,755 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:03:09,470 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:03:13,501 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:03:23,910 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  57%|█████▋    | 17/30 [19:43<15:25, 71.20s/iteration]\u001b[32m [ 2023-08-21 13:04:06,325 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:04:37,294 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  60%|██████    | 18/30 [20:59<14:32, 72.70s/iteration]\u001b[32m [ 2023-08-21 13:05:35,419 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:05:42,318 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:05:43,306 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:05:46,919 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  63%|██████▎   | 19/30 [22:09<13:11, 71.91s/iteration]\u001b[32m [ 2023-08-21 13:06:01,869 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:06:59,043 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  67%|██████▋   | 20/30 [23:27<12:16, 73.65s/iteration]\u001b[32m [ 2023-08-21 13:07:35,427 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:07:50,015 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:08:00,302 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  70%|███████   | 21/30 [24:35<10:48, 72.10s/iteration]\u001b[32m [ 2023-08-21 13:09:25,045 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  73%|███████▎  | 22/30 [25:49<09:40, 72.52s/iteration]\u001b[32m [ 2023-08-21 13:10:28,040 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:10:31,416 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  77%|███████▋  | 23/30 [27:00<08:24, 72.10s/iteration]\u001b[32m [ 2023-08-21 13:10:49,892 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:11:50,151 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  80%|████████  | 24/30 [28:10<07:09, 71.60s/iteration]\u001b[32m [ 2023-08-21 13:12:46,834 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:12:59,648 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:12:59,954 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  83%|████████▎ | 25/30 [29:18<05:52, 70.58s/iteration]\u001b[32m [ 2023-08-21 13:13:47,584 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:13:49,802 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:14:06,470 ] \u001b[0m2 individuals went zero\u001b[0m\n",
      "Iterations:  87%|████████▋ | 26/30 [30:27<04:39, 69.90s/iteration]\u001b[32m [ 2023-08-21 13:15:00,859 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  90%|█████████ | 27/30 [31:31<03:24, 68.25s/iteration]\u001b[32m [ 2023-08-21 13:16:09,628 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  93%|█████████▎| 28/30 [32:36<02:14, 67.34s/iteration]\u001b[32m [ 2023-08-21 13:17:00,783 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:17:03,331 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:17:11,471 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:17:13,470 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  97%|█████████▋| 29/30 [33:45<01:07, 67.82s/iteration]\u001b[32m [ 2023-08-21 13:18:28,475 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 13:18:30,439 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations: 100%|██████████| 30/30 [34:52<00:00, 69.74s/iteration]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 30\n",
    "da_results, da_features, da_dims, da_scores = run_da_optimization(num_iterations, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1cb2fa5-22fc-4630-bffd-b61cdec7b7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type'],\n",
       " ['age', 'work_type']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee01ba16-2cba-460b-bbd6-badafe2b4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stroke_data =pd.DataFrame(X[['age', 'work_type']])\n",
    "Stroke_data['stroke']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77e87a88-a0ac-469d-9040-8df5ad12b383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20230821_132030\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230821_132030\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   32.61 GB / 255.46 GB (12.8%)\n",
      "Train Data Rows:    9466\n",
      "Train Data Columns: 2\n",
      "Label Column: stroke\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1610.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.11 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1 | ['age']\n",
      "\t\t('int', [])   : 1 | ['work_type']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 1 | ['age']\n",
      "\t\t('int', [])   : 1 | ['work_type']\n",
      "\t0.1s = Fit runtime\n",
      "\t2 features in original data used to generate 2 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.11 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8519, Val Rows: 947\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9197\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9314\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.7761\t = Validation score   (accuracy)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9356\t = Validation score   (accuracy)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9461\t = Validation score   (accuracy)\n",
      "\t1.13s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9461\t = Validation score   (accuracy)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9388\t = Validation score   (accuracy)\n",
      "\t6.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.944\t = Validation score   (accuracy)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.943\t = Validation score   (accuracy)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t0.7645\t = Validation score   (accuracy)\n",
      "\t7.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.8553\t = Validation score   (accuracy)\n",
      "\t0.84s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7635\t = Validation score   (accuracy)\n",
      "\t20.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9345\t = Validation score   (accuracy)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9504\t = Validation score   (accuracy)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.89s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230821_132030\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor_combined = TabularPredictor(\n",
    "    label=\"stroke\", \n",
    "    \n",
    ").fit(\n",
    "    train_data=Stroke_data, \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45781d7d-074a-4eea-b7bb-e378fe2c9d6a",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d2be9cf-8b67-4c70-9bb3-15458a2440ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke3 = pd.read_csv('stroke/stroke3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "873c1da3-12ed-4024-9a0b-eb8dfa51f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke3=preprocess_dataset(stroke2)\n",
    "X = stroke3.drop(['stroke'],axis=1)\n",
    "y = stroke3['stroke']\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Assuming X and y are your feature matrix and target variable, respectively\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8c73e47-2cc3-49f8-9dd4-c9beaf103a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations:  13%|█▎        | 4/30 [35:51<3:57:06, 547.18s/iteration]\u001b[32m [ 2023-08-21 14:13:18,352 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  17%|█▋        | 5/30 [45:09<3:49:40, 551.23s/iteration]\u001b[32m [ 2023-08-21 14:14:23,784 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  20%|██        | 6/30 [54:06<3:38:31, 546.33s/iteration]\u001b[32m [ 2023-08-21 14:23:29,664 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 14:28:16,242 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 14:28:55,177 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  30%|███       | 9/30 [1:22:18<3:15:37, 558.93s/iteration]\u001b[32m [ 2023-08-21 14:58:23,488 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  40%|████      | 12/30 [1:49:22<2:46:22, 554.59s/iteration]\u001b[32m [ 2023-08-21 15:25:32,163 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  47%|████▋     | 14/30 [2:07:30<2:27:40, 553.77s/iteration]\u001b[32m [ 2023-08-21 15:38:59,989 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 15:42:18,262 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  53%|█████▎    | 16/30 [2:26:08<2:09:28, 554.91s/iteration]\u001b[32m [ 2023-08-21 15:54:54,171 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  60%|██████    | 18/30 [2:45:29<1:54:35, 572.95s/iteration]\u001b[32m [ 2023-08-21 16:22:00,534 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  73%|███████▎  | 22/30 [3:25:40<1:16:12, 571.62s/iteration]\u001b[32m [ 2023-08-21 16:58:10,011 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 17:00:31,035 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 17:01:51,730 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 17:02:30,574 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 17:02:59,529 ] \u001b[0m2 individuals went zero\u001b[0m\n",
      "\u001b[32m [ 2023-08-21 17:03:05,459 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations:  87%|████████▋ | 26/30 [19:16:34<13:24:05, 12061.49s/iteration]\u001b[32m [ 2023-08-22 08:54:48,474 ] \u001b[0m1 individuals went zero\u001b[0m\n",
      "Iterations: 100%|██████████| 30/30 [19:53:40<00:00, 2387.34s/iteration]    \n"
     ]
    }
   ],
   "source": [
    "num_iterations = 30 \n",
    "results, features, dims, scores = run_cgwoda_optimization(num_iterations, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1987c678-d5f3-4144-9af2-fa5bd5422164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['heart_disease', 'Residence_type', 'bmi'],\n",
       " ['gender', 'hypertension', 'bmi'],\n",
       " ['hypertension', 'heart_disease', 'ever_married', 'smoking_status'],\n",
       " ['heart_disease'],\n",
       " ['ever_married', 'bmi'],\n",
       " ['Residence_type', 'bmi'],\n",
       " ['Residence_type', 'bmi'],\n",
       " ['hypertension', 'heart_disease', 'ever_married', 'smoking_status'],\n",
       " ['hypertension', 'ever_married', 'work_type'],\n",
       " ['Residence_type', 'bmi'],\n",
       " ['age',\n",
       "  'ever_married',\n",
       "  'Residence_type',\n",
       "  'avg_glucose_level',\n",
       "  'bmi',\n",
       "  'smoking_status'],\n",
       " ['hypertension', 'bmi'],\n",
       " ['age',\n",
       "  'ever_married',\n",
       "  'Residence_type',\n",
       "  'avg_glucose_level',\n",
       "  'bmi',\n",
       "  'smoking_status'],\n",
       " ['Residence_type', 'bmi'],\n",
       " ['work_type'],\n",
       " ['age',\n",
       "  'ever_married',\n",
       "  'Residence_type',\n",
       "  'avg_glucose_level',\n",
       "  'bmi',\n",
       "  'smoking_status'],\n",
       " ['ever_married', 'work_type'],\n",
       " ['gender'],\n",
       " ['hypertension', 'bmi'],\n",
       " ['hypertension', 'bmi'],\n",
       " ['hypertension', 'ever_married'],\n",
       " ['heart_disease', 'Residence_type'],\n",
       " ['bmi'],\n",
       " ['heart_disease', 'ever_married', 'bmi'],\n",
       " ['gender', 'Residence_type', 'bmi'],\n",
       " ['hypertension', 'heart_disease', 'ever_married'],\n",
       " ['heart_disease', 'ever_married'],\n",
       " ['hypertension', 'heart_disease', 'work_type'],\n",
       " ['hypertension', 'ever_married', 'bmi'],\n",
       " ['hypertension', 'work_type']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bd0a006-f5a1-4cad-bbe7-1a0e09213a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stroke_data =pd.DataFrame(X[['age',\n",
    "  'ever_married',\n",
    "  'Residence_type',\n",
    "  'avg_glucose_level',\n",
    "  'bmi',\n",
    "  'smoking_status']])\n",
    "Stroke_data['stroke']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecb524e0-c899-4f76-8537-5061c772e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20230822_092619\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20230822_092619\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   31.29 GB / 255.46 GB (12.2%)\n",
      "Train Data Rows:    85234\n",
      "Train Data Columns: 6\n",
      "Label Column: stroke\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1548.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.07 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3 | ['age', 'avg_glucose_level', 'bmi']\n",
      "\t\t('int', [])   : 3 | ['ever_married', 'Residence_type', 'smoking_status']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 3 | ['age', 'avg_glucose_level', 'bmi']\n",
      "\t\t('int', [])       : 1 | ['smoking_status']\n",
      "\t\t('int', ['bool']) : 2 | ['ever_married', 'Residence_type']\n",
      "\t0.3s = Fit runtime\n",
      "\t6 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.56 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.029331018138301617, Train Rows: 82734, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9216\t = Validation score   (accuracy)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9288\t = Validation score   (accuracy)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.1088\n",
      "[2000]\tvalid_set's binary_error: 0.0784\n",
      "[3000]\tvalid_set's binary_error: 0.068\n",
      "[4000]\tvalid_set's binary_error: 0.0568\n",
      "[5000]\tvalid_set's binary_error: 0.0524\n",
      "[6000]\tvalid_set's binary_error: 0.0484\n",
      "[7000]\tvalid_set's binary_error: 0.0468\n",
      "[8000]\tvalid_set's binary_error: 0.046\n",
      "[9000]\tvalid_set's binary_error: 0.0432\n",
      "[10000]\tvalid_set's binary_error: 0.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9592\t = Validation score   (accuracy)\n",
      "\t50.87s\t = Training   runtime\n",
      "\t2.36s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.0256\n",
      "[2000]\tvalid_set's binary_error: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9784\t = Validation score   (accuracy)\n",
      "\t11.48s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 230 due to low memory. Expected memory usage reduced from 19.52% -> 15.0% of available memory...\n",
      "\t0.9756\t = Validation score   (accuracy)\n",
      "\t17.27s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 283 due to low memory. Expected memory usage reduced from 15.87% -> 15.0% of available memory...\n",
      "\t0.9748\t = Validation score   (accuracy)\n",
      "\t29.85s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9796\t = Validation score   (accuracy)\n",
      "\t110.91s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 119 due to low memory. Expected memory usage reduced from 37.55% -> 15.0% of available memory...\n",
      "\t0.972\t = Validation score   (accuracy)\n",
      "\t3.6s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 119 due to low memory. Expected memory usage reduced from 37.76% -> 15.0% of available memory...\n",
      "\t0.9692\t = Validation score   (accuracy)\n",
      "\t3.6s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.8596\t = Validation score   (accuracy)\n",
      "\t145.33s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9684\t = Validation score   (accuracy)\n",
      "\t10.6s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.926\t = Validation score   (accuracy)\n",
      "\t780.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9764\t = Validation score   (accuracy)\n",
      "\t6.8s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9844\t = Validation score   (accuracy)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1183.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20230822_092619\\\")\n"
     ]
    }
   ],
   "source": [
    "predictor_combined = TabularPredictor(\n",
    "    label=\"stroke\", \n",
    "    \n",
    ").fit(\n",
    "    train_data=Stroke_data, \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b10bb6e1-4d75-45f9-a3aa-6ae614b2f5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2     0.9844       3.022124  209.405931                0.004718           1.470291            2       True         14\n",
      "1              CatBoost     0.9796       0.018949  110.913519                0.018949         110.913519            1       True          7\n",
      "2              LightGBM     0.9784       0.183970   11.477721                0.183970          11.477721            1       True          4\n",
      "3         LightGBMLarge     0.9764       0.138153    6.802608                0.138153           6.802608            1       True         13\n",
      "4      RandomForestGini     0.9756       0.184586   17.273694                0.184586          17.273694            1       True          5\n",
      "5      RandomForestEntr     0.9748       0.183279   29.845453                0.183279          29.845453            1       True          6\n",
      "6        ExtraTreesGini     0.9720       0.103960    3.604559                0.103960           3.604559            1       True          8\n",
      "7        ExtraTreesEntr     0.9692       0.075672    3.604416                0.075672           3.604416            1       True          9\n",
      "8               XGBoost     0.9684       0.129939   10.595365                0.129939          10.595365            1       True         11\n",
      "9            LightGBMXT     0.9592       2.361809   50.872734                2.361809          50.872734            1       True          3\n",
      "10       KNeighborsDist     0.9288       0.016952    0.070491                0.016952           0.070491            1       True          2\n",
      "11       NeuralNetTorch     0.9260       0.003099  780.966259                0.003099         780.966259            1       True         12\n",
      "12       KNeighborsUnif     0.9216       0.044506    0.132656                0.044506           0.132656            1       True          1\n",
      "13      NeuralNetFastAI     0.8596       0.074798  145.329655                0.074798         145.329655            1       True         10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>3.022124</td>\n",
       "      <td>209.405931</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>1.470291</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.018949</td>\n",
       "      <td>110.913519</td>\n",
       "      <td>0.018949</td>\n",
       "      <td>110.913519</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>0.183970</td>\n",
       "      <td>11.477721</td>\n",
       "      <td>0.183970</td>\n",
       "      <td>11.477721</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBMLarge</td>\n",
       "      <td>0.9764</td>\n",
       "      <td>0.138153</td>\n",
       "      <td>6.802608</td>\n",
       "      <td>0.138153</td>\n",
       "      <td>6.802608</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestGini</td>\n",
       "      <td>0.9756</td>\n",
       "      <td>0.184586</td>\n",
       "      <td>17.273694</td>\n",
       "      <td>0.184586</td>\n",
       "      <td>17.273694</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestEntr</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.183279</td>\n",
       "      <td>29.845453</td>\n",
       "      <td>0.183279</td>\n",
       "      <td>29.845453</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTreesGini</td>\n",
       "      <td>0.9720</td>\n",
       "      <td>0.103960</td>\n",
       "      <td>3.604559</td>\n",
       "      <td>0.103960</td>\n",
       "      <td>3.604559</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ExtraTreesEntr</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.075672</td>\n",
       "      <td>3.604416</td>\n",
       "      <td>0.075672</td>\n",
       "      <td>3.604416</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>0.129939</td>\n",
       "      <td>10.595365</td>\n",
       "      <td>0.129939</td>\n",
       "      <td>10.595365</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LightGBMXT</td>\n",
       "      <td>0.9592</td>\n",
       "      <td>2.361809</td>\n",
       "      <td>50.872734</td>\n",
       "      <td>2.361809</td>\n",
       "      <td>50.872734</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>0.016952</td>\n",
       "      <td>0.070491</td>\n",
       "      <td>0.016952</td>\n",
       "      <td>0.070491</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NeuralNetTorch</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>780.966259</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>780.966259</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>0.9216</td>\n",
       "      <td>0.044506</td>\n",
       "      <td>0.132656</td>\n",
       "      <td>0.044506</td>\n",
       "      <td>0.132656</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NeuralNetFastAI</td>\n",
       "      <td>0.8596</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>145.329655</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>145.329655</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  score_val  pred_time_val    fit_time  \\\n",
       "0   WeightedEnsemble_L2     0.9844       3.022124  209.405931   \n",
       "1              CatBoost     0.9796       0.018949  110.913519   \n",
       "2              LightGBM     0.9784       0.183970   11.477721   \n",
       "3         LightGBMLarge     0.9764       0.138153    6.802608   \n",
       "4      RandomForestGini     0.9756       0.184586   17.273694   \n",
       "5      RandomForestEntr     0.9748       0.183279   29.845453   \n",
       "6        ExtraTreesGini     0.9720       0.103960    3.604559   \n",
       "7        ExtraTreesEntr     0.9692       0.075672    3.604416   \n",
       "8               XGBoost     0.9684       0.129939   10.595365   \n",
       "9            LightGBMXT     0.9592       2.361809   50.872734   \n",
       "10       KNeighborsDist     0.9288       0.016952    0.070491   \n",
       "11       NeuralNetTorch     0.9260       0.003099  780.966259   \n",
       "12       KNeighborsUnif     0.9216       0.044506    0.132656   \n",
       "13      NeuralNetFastAI     0.8596       0.074798  145.329655   \n",
       "\n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                 0.004718           1.470291            2       True   \n",
       "1                 0.018949         110.913519            1       True   \n",
       "2                 0.183970          11.477721            1       True   \n",
       "3                 0.138153           6.802608            1       True   \n",
       "4                 0.184586          17.273694            1       True   \n",
       "5                 0.183279          29.845453            1       True   \n",
       "6                 0.103960           3.604559            1       True   \n",
       "7                 0.075672           3.604416            1       True   \n",
       "8                 0.129939          10.595365            1       True   \n",
       "9                 2.361809          50.872734            1       True   \n",
       "10                0.016952           0.070491            1       True   \n",
       "11                0.003099         780.966259            1       True   \n",
       "12                0.044506           0.132656            1       True   \n",
       "13                0.074798         145.329655            1       True   \n",
       "\n",
       "    fit_order  \n",
       "0          14  \n",
       "1           7  \n",
       "2           4  \n",
       "3          13  \n",
       "4           5  \n",
       "5           6  \n",
       "6           8  \n",
       "7           9  \n",
       "8          11  \n",
       "9           3  \n",
       "10          2  \n",
       "11         12  \n",
       "12          1  \n",
       "13         10  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_combined.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40090ca-d0f4-4288-83ce-f43a22eedff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
